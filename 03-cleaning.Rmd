# Data transformation

```{r echo=FALSE,message=FALSE}
library(tidyverse)
library(dplyr)
homevalue_city <- read.csv(file = 'data/homevalue_city_clean.csv')
```


```{r echo=FALSE}
homevalue2010 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2010),list(X2010 = mean))
homevalue2011 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2011),list(X2011 = mean))
homevalue <- cbind(homevalue2010, X2011=homevalue2011$X2011)

homevalue2012 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2012),list(X2012 = mean))
homevalue <- cbind(homevalue, X2012=homevalue2012$X2012)

homevalue2013 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2013),list(X2013 = mean))
homevalue <- cbind(homevalue, X2013=homevalue2013$X2013)

homevalue2014 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2014),list(X2014 = mean))
homevalue <- cbind(homevalue, X2014=homevalue2014$X2014)

homevalue2015 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2015),list(X2015 = mean))
homevalue <- cbind(homevalue, X2015=homevalue2015$X2015)

homevalue2016 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2016),list(X2016 = mean))
homevalue <- cbind(homevalue, X2016=homevalue2016$X2016)

homevalue2017 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2017),list(X2017 = mean))
homevalue <- cbind(homevalue, X2017=homevalue2017$X2017)

homevalue2018 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2018),list(X2018 = mean))
homevalue <- cbind(homevalue, X2017=homevalue2017$X2017)

homevalue2019 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2019),list(X2019 = mean))
homevalue <- cbind(homevalue, X2019=homevalue2019$X2019)

write.csv(homevalue,"data/home_value_state.csv", row.names = TRUE)

```

Since we have a large amount of data from multiple sources, we decided to clean the data (details about methodology can be found in Chapter 4: Missing value) and build two data sets for our visualization. 

The first data set contains the yearly average home value data for each state. Given that the raw data contains city level home values, we summarized to the state level by averaging the home value of all the cities in that state. Furthermore, we also summarized our monthly data to yearly data by averaging data acorss 12 months into a specific year. (Note averaging across the 12 months also standardized seasonality in the data.) The result is a home value data set for each state from 2010-2019 that includes state name, state initial and home value for 10 years.

The second data set we generated is the mixed data set for all our economic variables that may have an effect on home value as we listed before. Since we have 5 variables (state, home value, income, unemployment, and population), we collected 10 years worth of data and then transformed it from 3D to 2D. We used pivot-longer to increase the number of rows by transforming 'Year' as a column to keep in line with tidy principles. 

Finally with these 2 data sets, we decided to further combine them so that each row of the data set would contain the following 8 variables: Area (state full name), State Name (state initial), Year, Home value, Population, Housing Unit, Unemployment rate, and Income. This final data set would contain information from all 50 States across the years 2010 to 2019.


