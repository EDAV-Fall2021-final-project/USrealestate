# Data transformation

```{r echo=FALSE,message=FALSE}
library(tidyverse)
library(dplyr)
homevalue_city <- read.csv(file = 'data/homevalue_city_clean.csv')
```


```{r echo=FALSE}
homevalue2010 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2010),list(X2010 = mean))
homevalue2011 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2011),list(X2011 = mean))
homevalue <- cbind(homevalue2010, X2011=homevalue2011$X2011)

homevalue2012 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2012),list(X2012 = mean))
homevalue <- cbind(homevalue, X2012=homevalue2012$X2012)

homevalue2013 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2013),list(X2013 = mean))
homevalue <- cbind(homevalue, X2013=homevalue2013$X2013)

homevalue2014 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2014),list(X2014 = mean))
homevalue <- cbind(homevalue, X2014=homevalue2014$X2014)

homevalue2015 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2015),list(X2015 = mean))
homevalue <- cbind(homevalue, X2015=homevalue2015$X2015)

homevalue2016 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2016),list(X2016 = mean))
homevalue <- cbind(homevalue, X2016=homevalue2016$X2016)

homevalue2017 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2017),list(X2017 = mean))
homevalue <- cbind(homevalue, X2017=homevalue2017$X2017)

homevalue2018 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2018),list(X2018 = mean))
homevalue <- cbind(homevalue, X2017=homevalue2017$X2017)

homevalue2019 <- homevalue_city %>% group_by(homevalue_city$StateName) %>% 
  summarise_at(vars(X2019),list(X2019 = mean))
homevalue <- cbind(homevalue, X2019=homevalue2019$X2019)

write.csv(homevalue,"data/home_value_state.csv", row.names = TRUE)

```

Since we have a large amount of data from multiple sources, we decided to clen the data (details about methodology can be found in Chapter 4: Missing value) and build two data sets for our visualization. 

The first one is the data set that contain the yearly average home value data for each state. To summaries the home value of each city into each state, we calculated the home value in a certain year by averaging the home value of all the cities in that state. Similarly, to summarize our monthly data to yearly data, we averaged home value across the 12 months in a year and stored it as the yearly home value. The result is a home value data set for each state from 2010-2019 that includes state name, state initial and home value for 10 years.

The second dataset we generated is the mixed data set for all our economic variables that may have an effect on home value as we listed before. Since we have 5 variables (state, home value, income, unemployment, and population), we collected 10 years worth of data and then transformed it from 3D to 2D. We used pivot-longer to increase the number of rows by transforming 'Year' as a column to keep in line with tidy principles. 

Finally with these 2 data sets, we decided to further combine them so that each row of the data set would contain the following 8 variables: Area (state full name), State Name (state initial), Year, Home value, Population, Housing Unit, Unemployment rate, and Income. This final dataset would contain informatiom from all 50 States across the years 2010 to 2019.


